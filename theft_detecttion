{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 451400,
          "sourceType": "datasetVersion",
          "datasetId": 205791
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishitasingh21/Credit-Card-Fraud-Detection/blob/main/theft_detecttion\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'dcsass-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F205791%2F451400%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240425%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240425T122731Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dc046f3453794dc98fc831cee78614fccb5105927594cb3974ffdcb9616d7fa4e92dc08099e0ad52cd6e6029d82b91215e638bf690486964d0bf164c88d7cce9d647a20a812eb931488269dd61246d765c7150363826c7a91aa657bb55b14b734f0948b8689b548396ce2fb9721838347f446c1ed55e5ce5d2b57c8b2c80430286af90a26b25850635a5f7f69d982ff963100a16f690af5e22ada0ec3b3a1fb65ef8bdf28ca60a6905dff87d0fbc0173bf61e4e1e7172f69c818560d9cc682590f09af789462f54f33db455806c062ddd23e53f44ba15e111e62ceb173b0c113bef5d05c32cd8bfd2209343ffaccf46424d4d20d7a9469a4410fce56c04173386'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "aOqcR3EoObUF",
        "outputId": "140abe4c-e542-403f-adf3-3997e53ae6da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dcsass-dataset, 1453262946 bytes compressed\n",
            "[==================================================] 1453262946 bytes downloaded\n",
            "Downloaded and uncompressed: dcsass-dataset\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataframe\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/kaggle/input/dcsass-dataset/DCSASS Dataset/Labels/Shoplifting.csv')\n",
        "df"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-19T15:51:29.730386Z",
          "iopub.execute_input": "2024-04-19T15:51:29.730659Z",
          "iopub.status.idle": "2024-04-19T15:51:30.729334Z",
          "shell.execute_reply.started": "2024-04-19T15:51:29.730634Z",
          "shell.execute_reply": "2024-04-19T15:51:30.728248Z"
        },
        "trusted": true,
        "id": "-qeBZfFQObUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Shoplifting001_x264_0'] = df['Shoplifting001_x264_0'] + \".mp4\"\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-19T15:51:30.731422Z",
          "iopub.execute_input": "2024-04-19T15:51:30.731822Z",
          "iopub.status.idle": "2024-04-19T15:51:30.74702Z",
          "shell.execute_reply.started": "2024-04-19T15:51:30.731786Z",
          "shell.execute_reply": "2024-04-19T15:51:30.745828Z"
        },
        "trusted": true,
        "id": "G8FvEMAXObUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making directory to store the temporary data which will be used later\n",
        "\n",
        "import os\n",
        "\n",
        "os.mkdir('/kaggle/working/data')\n",
        "os.mkdir('/kaggle/working/data/0')\n",
        "os.mkdir('/kaggle/working/data/1')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-19T15:51:30.74873Z",
          "iopub.execute_input": "2024-04-19T15:51:30.749106Z",
          "iopub.status.idle": "2024-04-19T15:51:30.755711Z",
          "shell.execute_reply.started": "2024-04-19T15:51:30.74907Z",
          "shell.execute_reply": "2024-04-19T15:51:30.754765Z"
        },
        "trusted": true,
        "id": "6w70n44QObUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we can see that the videos are of 4 seconds each so using 3d convolutions would be useless we would have to process the data on individual frames\n",
        "# also some videos are of less than 4 seconds and padding 1 second video to 4 second would not make any sense\n",
        "\n",
        "import cv2\n",
        "video = cv2.VideoCapture('/kaggle/input/dcsass-dataset/DCSASS Dataset/Shoplifting/Shoplifting001_x264.mp4/Shoplifting001_x264_1.mp4')\n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "\n",
        "print(f\"Video Resolution: {width}x{height}\")\n",
        "print(f\"Video FPS: {fps}\")\n",
        "print(f\"Total Frames: {total_frames}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-19T15:51:30.758473Z",
          "iopub.execute_input": "2024-04-19T15:51:30.758745Z",
          "iopub.status.idle": "2024-04-19T15:51:31.063651Z",
          "shell.execute_reply.started": "2024-04-19T15:51:30.758722Z",
          "shell.execute_reply": "2024-04-19T15:51:31.062532Z"
        },
        "trusted": true,
        "id": "4cwx3M6lObUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the individual frames into two folders 0 and 1 in the working directory of kaggle and making sure that the labels of 0 class are almost equal to the 1 class of data\n",
        "\n",
        "frame_num = 1\n",
        "zero_cnt = 0\n",
        "one_cnt = 0\n",
        "for i in range(len(df)):\n",
        "    path = df[\"Shoplifting001_x264_0\"].iloc[i]\n",
        "    path = \"/kaggle/input/dcsass-dataset/DCSASS Dataset/Shoplifting/\" + path[:19] +\".mp4/\" +path\n",
        "    video = cv2.VideoCapture(path)\n",
        "    label = df[\"0\"].iloc[i]\n",
        "    ret,frame = video.read()\n",
        "    frame_cnt = 1\n",
        "    while ret:\n",
        "        if frame_cnt%30==0:\n",
        "            if label == 1:\n",
        "                cv2.imwrite(f'/kaggle/working/data/{label}/f{frame_num}.jpg',frame)\n",
        "                one_cnt+=1\n",
        "            else:\n",
        "                if zero_cnt<one_cnt or one_cnt<100:\n",
        "                    cv2.imwrite(f'/kaggle/working/data/{label}/f{frame_num}.jpg',frame)\n",
        "                    zero_cnt+=1\n",
        "            frame_num+=1\n",
        "        frame_cnt+=1\n",
        "        ret,frame = video.read()"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-19T15:51:31.065349Z",
          "iopub.execute_input": "2024-04-19T15:51:31.06571Z",
          "iopub.status.idle": "2024-04-19T15:51:54.719184Z",
          "shell.execute_reply.started": "2024-04-19T15:51:31.065678Z",
          "shell.execute_reply": "2024-04-19T15:51:54.718341Z"
        },
        "trusted": true,
        "id": "SxDq7bwRObUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Now we make data generator so that we can use images stored in the directories to train ml Model</h1>\n"
      ],
      "metadata": {
        "id": "oUpQnSzPObUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "root_dir = '/kaggle/working/data'\n",
        "datagen = ImageDataGenerator(\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    root_dir,\n",
        "    target_size=(320, 240),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    root_dir,\n",
        "    target_size=(320,240),\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-19T15:51:54.720426Z",
          "iopub.execute_input": "2024-04-19T15:51:54.720739Z",
          "iopub.status.idle": "2024-04-19T15:52:06.668956Z",
          "shell.execute_reply.started": "2024-04-19T15:51:54.720714Z",
          "shell.execute_reply": "2024-04-19T15:52:06.668209Z"
        },
        "trusted": true,
        "id": "cfXgC4goObUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making ml model to do binary classification here we do transfer learning on imagenet model\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "base_model = keras.applications.MobileNetV2(include_top=False, input_shape=(320, 240,3))\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "checkpoint_path = \"/kaggle/working/model.weights.h5\"\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    monitor='val_accuracy',\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    mode='max')\n",
        "\n",
        "# # Compile the model\n",
        "model.compile(optimizer=Nadam(weight_decay=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Reduce learning rate when a metric has stopped improving\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-19T15:52:06.670051Z",
          "iopub.execute_input": "2024-04-19T15:52:06.670939Z",
          "iopub.status.idle": "2024-04-19T15:52:08.730307Z",
          "shell.execute_reply.started": "2024-04-19T15:52:06.670903Z",
          "shell.execute_reply": "2024-04-19T15:52:08.72956Z"
        },
        "trusted": true,
        "id": "Aw30I6inObUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "\n",
        "epochs = 10\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    callbacks = [reduce_lr,checkpoint_callback]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-19T15:52:08.731304Z",
          "iopub.execute_input": "2024-04-19T15:52:08.731555Z",
          "iopub.status.idle": "2024-04-19T15:54:51.017091Z",
          "shell.execute_reply.started": "2024-04-19T15:52:08.731533Z",
          "shell.execute_reply": "2024-04-19T15:54:51.016302Z"
        },
        "trusted": true,
        "id": "FigopgxeObUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>here we can see that the best performance of the model was 90 percent and we saved it's weights automatically and then the model just started to overfit and the accuracy decreases drastically</h1>"
      ],
      "metadata": {
        "id": "2VwH2M77ObUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the best weihts of the model\n",
        "model.load_weights('/kaggle/working/model.weights.h5')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-19T15:54:51.021605Z",
          "iopub.execute_input": "2024-04-19T15:54:51.021892Z",
          "iopub.status.idle": "2024-04-19T15:54:52.6627Z",
          "shell.execute_reply.started": "2024-04-19T15:54:51.021867Z",
          "shell.execute_reply": "2024-04-19T15:54:52.661903Z"
        },
        "trusted": true,
        "id": "99BAcDAlObUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Evaluating the model</h1>"
      ],
      "metadata": {
        "id": "bihN4OZLObUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, f1_score, top_k_accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# # Evaluate the model on the validation set\n",
        "predictions = model.predict(val_generator)\n",
        "\n",
        "\n",
        "# Convert true labels to class labels\n",
        "true_labels = val_generator.classes\n",
        "predictions  = predictions >0.5\n",
        "predictions = predictions.astype(int)\n",
        "predictions = predictions.flatten()\n",
        "# # Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(true_labels,predictions)\n",
        "\n",
        "# # Calculate recall, F1 score, and classification report\n",
        "recall = recall_score(true_labels, predictions, average='weighted')\n",
        "f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "classification_rep = classification_report(true_labels, predictions)\n",
        "\n",
        "\n",
        "print(f'Overall Accuracy: {overall_accuracy * 100:.2f}%')\n",
        "print(f'Recall: {recall * 100:.2f}%')\n",
        "print(f'F1 Score: {f1 * 100:.2f}%')\n",
        "print(conf_matrix)\n",
        "print('Classification Report:')\n",
        "print(classification_rep)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-19T15:54:52.66595Z",
          "iopub.execute_input": "2024-04-19T15:54:52.666353Z",
          "iopub.status.idle": "2024-04-19T15:54:59.709295Z",
          "shell.execute_reply.started": "2024-04-19T15:54:52.666326Z",
          "shell.execute_reply": "2024-04-19T15:54:59.708318Z"
        },
        "trusted": true,
        "id": "o_p4DtK3ObUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(15,15))\n",
        "    sns.set(font_scale=1.2)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, linewidths=.5, square=True,)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Replace y_true and y_pred with your actual and predicted labels\n",
        "\n",
        "plot_confusion_matrix(true_labels,predictions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-19T15:54:59.710421Z",
          "iopub.execute_input": "2024-04-19T15:54:59.710763Z",
          "iopub.status.idle": "2024-04-19T15:55:00.279116Z",
          "shell.execute_reply.started": "2024-04-19T15:54:59.710732Z",
          "shell.execute_reply": "2024-04-19T15:55:00.278142Z"
        },
        "trusted": true,
        "id": "qhRp4riaObUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFT82LvcObUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}